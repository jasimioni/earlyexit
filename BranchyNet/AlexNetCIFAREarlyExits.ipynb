{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d6aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from models.AlexNet import AlexNet, AlexNetCIFAR10, AlexNetCIFAR10ee1, AlexNetCIFAR10ee2, AlexNetWithExistsCIFAR10\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3783bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 600\n",
    "\n",
    "train_data   = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data    = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a0d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetWithExistsCIFAR10(\n",
      "  (backbone): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (exits): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Flatten(start_dim=1, end_dim=-1)\n",
      "        (1): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Flatten(start_dim=1, end_dim=-1)\n",
      "        (1): Linear(in_features=2304, out_features=10, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (1): Sequential(\n",
      "        (0): Dropout(p=0.5, inplace=False)\n",
      "        (1): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Dropout(p=0.5, inplace=False)\n",
      "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (3): Linear(in_features=1024, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetWithExistsCIFAR10(exit_loss_weights=[20, 5, 1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    X_train = X_train.to(device)\n",
    "#    y_train = y_train.to(device)\n",
    "    break\n",
    "\n",
    "print(X_train.shape)\n",
    "    \n",
    "x = X_train.view(1,3,32,32)\n",
    "print(x.shape)\n",
    "results = model.forward(x)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02b41ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:    1 [   600/60000]  loss: 0.34898761  accuracy train:  88.333%\n",
      "epoch:  0  batch:   11 [  6600/60000]  loss: 0.48136407  accuracy train:  82.864%\n",
      "epoch:  0  batch:   21 [ 12600/60000]  loss: 0.48491627  accuracy train:  83.937%\n",
      "epoch:  0  batch:   31 [ 18600/60000]  loss: 0.39352533  accuracy train:  84.651%\n",
      "epoch:  0  batch:   41 [ 24600/60000]  loss: 0.38678420  accuracy train:  85.293%\n",
      "epoch:  0  batch:   51 [ 30600/60000]  loss: 0.40199876  accuracy train:  85.745%\n",
      "epoch:  0  batch:   61 [ 36600/60000]  loss: 0.32434300  accuracy train:  85.921%\n",
      "epoch:  0  batch:   71 [ 42600/60000]  loss: 0.41014165  accuracy train:  86.002%\n",
      "epoch:  0  batch:   81 [ 48600/60000]  loss: 0.32028916  accuracy train:  86.117%\n",
      "\n",
      "Loss: 0.67486274 - Accuracy Test: 80.98\n",
      "\n",
      "epoch:  1  batch:    1 [   600/60000]  loss: 0.34843138  accuracy train:  88.333%\n",
      "epoch:  1  batch:   11 [  6600/60000]  loss: 0.29306197  accuracy train:  88.455%\n",
      "epoch:  1  batch:   21 [ 12600/60000]  loss: 0.33086836  accuracy train:  89.095%\n",
      "epoch:  1  batch:   31 [ 18600/60000]  loss: 0.30743948  accuracy train:  89.005%\n",
      "epoch:  1  batch:   41 [ 24600/60000]  loss: 0.29649019  accuracy train:  89.057%\n",
      "epoch:  1  batch:   51 [ 30600/60000]  loss: 0.37961978  accuracy train:  88.814%\n",
      "epoch:  1  batch:   61 [ 36600/60000]  loss: 0.26625553  accuracy train:  88.650%\n",
      "epoch:  1  batch:   71 [ 42600/60000]  loss: 0.32256466  accuracy train:  88.608%\n",
      "epoch:  1  batch:   81 [ 48600/60000]  loss: 0.35195979  accuracy train:  88.527%\n",
      "\n",
      "Loss: 0.68085128 - Accuracy Test: 81.77\n",
      "\n",
      "epoch:  2  batch:    1 [   600/60000]  loss: 0.29530334  accuracy train:  90.500%\n",
      "epoch:  2  batch:   11 [  6600/60000]  loss: 0.31538421  accuracy train:  90.333%\n",
      "epoch:  2  batch:   21 [ 12600/60000]  loss: 0.22146253  accuracy train:  90.333%\n",
      "epoch:  2  batch:   31 [ 18600/60000]  loss: 0.26220772  accuracy train:  90.398%\n",
      "epoch:  2  batch:   41 [ 24600/60000]  loss: 0.32833368  accuracy train:  90.228%\n",
      "epoch:  2  batch:   51 [ 30600/60000]  loss: 0.28362393  accuracy train:  90.193%\n",
      "epoch:  2  batch:   61 [ 36600/60000]  loss: 0.29218769  accuracy train:  89.962%\n",
      "epoch:  2  batch:   71 [ 42600/60000]  loss: 0.24667330  accuracy train:  89.800%\n",
      "epoch:  2  batch:   81 [ 48600/60000]  loss: 0.28533357  accuracy train:  89.745%\n",
      "\n",
      "Loss: 0.68508172 - Accuracy Test: 80.73\n",
      "\n",
      "epoch:  3  batch:    1 [   600/60000]  loss: 0.28668475  accuracy train:  89.000%\n",
      "epoch:  3  batch:   11 [  6600/60000]  loss: 0.25237334  accuracy train:  90.061%\n",
      "epoch:  3  batch:   21 [ 12600/60000]  loss: 0.25122452  accuracy train:  90.738%\n",
      "epoch:  3  batch:   31 [ 18600/60000]  loss: 0.23144920  accuracy train:  91.102%\n",
      "epoch:  3  batch:   41 [ 24600/60000]  loss: 0.26229104  accuracy train:  91.138%\n",
      "epoch:  3  batch:   51 [ 30600/60000]  loss: 0.27334678  accuracy train:  90.967%\n",
      "epoch:  3  batch:   61 [ 36600/60000]  loss: 0.21639872  accuracy train:  90.874%\n",
      "epoch:  3  batch:   71 [ 42600/60000]  loss: 0.25404283  accuracy train:  90.805%\n",
      "epoch:  3  batch:   81 [ 48600/60000]  loss: 0.30712324  accuracy train:  90.829%\n",
      "\n",
      "Loss: 0.66557550 - Accuracy Test: 81.68\n",
      "\n",
      "epoch:  4  batch:    1 [   600/60000]  loss: 0.25245810  accuracy train:  90.833%\n",
      "epoch:  4  batch:   11 [  6600/60000]  loss: 0.23645040  accuracy train:  91.197%\n",
      "epoch:  4  batch:   21 [ 12600/60000]  loss: 0.23991244  accuracy train:  91.484%\n",
      "epoch:  4  batch:   31 [ 18600/60000]  loss: 0.26711035  accuracy train:  91.629%\n",
      "epoch:  4  batch:   41 [ 24600/60000]  loss: 0.27498478  accuracy train:  91.715%\n",
      "epoch:  4  batch:   51 [ 30600/60000]  loss: 0.22802587  accuracy train:  91.657%\n",
      "epoch:  4  batch:   61 [ 36600/60000]  loss: 0.23632453  accuracy train:  91.593%\n",
      "epoch:  4  batch:   71 [ 42600/60000]  loss: 0.22801583  accuracy train:  91.709%\n",
      "epoch:  4  batch:   81 [ 48600/60000]  loss: 0.19965419  accuracy train:  91.673%\n",
      "\n",
      "Loss: 0.67816317 - Accuracy Test: 82.04\n",
      "\n",
      "\n",
      "Duration: 86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train backbone\n",
    "\n",
    "backbone_params = [\n",
    "      {'params': model.backbone.parameters()},\n",
    "      {'params': model.exits[-1].parameters()}\n",
    "]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(backbone_params, lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)[-1]  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if (b-1)%10 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy train: {trn_corr.item()*100/(batch_size*b):7.3f}%')\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test)[-1]\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            test_count += len(predicted)\n",
    "            \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "    \n",
    "    print(f\"\\nLoss: {loss.item():10.8f} - Accuracy Test: {100*tst_corr/test_count:2.2f}\\n\")\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69dfcaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 5, 1]\n",
      "epoch:  0  batch:    1 [   600/60000]  losses: ['8.50893593', '0.93021142', '0.24012361']  accuracy train: [' 85.833%', ' 93.333%', ' 91.833%']\n",
      "epoch:  0  batch:   11 [  6600/60000]  losses: ['11.97224426', '1.82826269', '0.29086778']  accuracy train: [' 78.152%', ' 84.682%', ' 90.106%']\n",
      "epoch:  0  batch:   21 [ 12600/60000]  losses: ['10.89184380', '1.25134408', '0.20737654']  accuracy train: [' 79.802%', ' 87.452%', ' 90.365%']\n",
      "epoch:  0  batch:   31 [ 18600/60000]  losses: ['8.69488144', '0.95371747', '0.19286132']  accuracy train: [' 81.027%', ' 89.285%', ' 91.145%']\n",
      "epoch:  0  batch:   41 [ 24600/60000]  losses: ['8.38115025', '0.88418013', '0.17516689']  accuracy train: [' 81.874%', ' 90.329%', ' 91.780%']\n",
      "epoch:  0  batch:   51 [ 30600/60000]  losses: ['9.17456436', '0.83370042', '0.18573123']  accuracy train: [' 82.458%', ' 91.029%', ' 92.160%']\n",
      "epoch:  0  batch:   61 [ 36600/60000]  losses: ['9.36941719', '1.00042641', '0.20607631']  accuracy train: [' 82.967%', ' 91.533%', ' 92.407%']\n",
      "epoch:  0  batch:   71 [ 42600/60000]  losses: ['7.94903946', '0.77465487', '0.14942867']  accuracy train: [' 83.242%', ' 91.782%', ' 92.549%']\n",
      "epoch:  0  batch:   81 [ 48600/60000]  losses: ['9.07777214', '0.98627853', '0.16684832']  accuracy train: [' 83.549%', ' 91.979%', ' 92.642%']\n",
      "\n",
      "Loss: ['0.63748473', '0.52748442', '0.69587225'] - Accuracy Test: ['78.08', '83.88', '83.45']\n",
      "\n",
      "epoch:  1  batch:    1 [   600/60000]  losses: ['7.72062492', '0.81053066', '0.16125044']  accuracy train: [' 87.500%', ' 94.500%', ' 94.167%']\n",
      "epoch:  1  batch:   11 [  6600/60000]  losses: ['6.86928844', '0.58923018', '0.13879390']  accuracy train: [' 87.242%', ' 95.606%', ' 94.970%']\n",
      "epoch:  1  batch:   21 [ 12600/60000]  losses: ['7.42015076', '0.65485871', '0.15540695']  accuracy train: [' 87.524%', ' 95.794%', ' 94.897%']\n",
      "epoch:  1  batch:   31 [ 18600/60000]  losses: ['6.66914511', '0.59294981', '0.10531157']  accuracy train: [' 87.640%', ' 95.978%', ' 94.978%']\n",
      "epoch:  1  batch:   41 [ 24600/60000]  losses: ['6.63775921', '0.68570840', '0.15780988']  accuracy train: [' 87.748%', ' 95.846%', ' 94.866%']\n",
      "epoch:  1  batch:   51 [ 30600/60000]  losses: ['7.42674589', '0.75716269', '0.15563484']  accuracy train: [' 87.578%', ' 95.709%', ' 94.758%']\n",
      "epoch:  1  batch:   61 [ 36600/60000]  losses: ['6.47012234', '0.88441765', '0.17329231']  accuracy train: [' 87.593%', ' 95.546%', ' 94.710%']\n",
      "epoch:  1  batch:   71 [ 42600/60000]  losses: ['6.96732759', '0.76693743', '0.16439979']  accuracy train: [' 87.545%', ' 95.397%', ' 94.624%']\n",
      "epoch:  1  batch:   81 [ 48600/60000]  losses: ['7.78544617', '0.74622595', '0.15881173']  accuracy train: [' 87.403%', ' 95.226%', ' 94.556%']\n",
      "\n",
      "Loss: ['0.65052980', '0.57226843', '0.69838661'] - Accuracy Test: ['79.18', '83.34', '83.25']\n",
      "\n",
      "epoch:  2  batch:    1 [   600/60000]  losses: ['5.89173985', '0.78204089', '0.14737584']  accuracy train: [' 91.167%', ' 96.167%', ' 95.500%']\n",
      "epoch:  2  batch:   11 [  6600/60000]  losses: ['6.31094503', '0.53739512', '0.13936228']  accuracy train: [' 89.879%', ' 96.121%', ' 94.924%']\n",
      "epoch:  2  batch:   21 [ 12600/60000]  losses: ['6.28522444', '0.59051389', '0.12067084']  accuracy train: [' 90.103%', ' 96.302%', ' 95.159%']\n",
      "epoch:  2  batch:   31 [ 18600/60000]  losses: ['6.25036907', '0.40657336', '0.12869971']  accuracy train: [' 90.102%', ' 96.478%', ' 95.263%']\n",
      "epoch:  2  batch:   41 [ 24600/60000]  losses: ['5.60175657', '0.46178913', '0.13251895']  accuracy train: [' 90.199%', ' 96.549%', ' 95.305%']\n",
      "epoch:  2  batch:   51 [ 30600/60000]  losses: ['7.00823736', '0.68413663', '0.17382309']  accuracy train: [' 90.023%', ' 96.425%', ' 95.255%']\n",
      "epoch:  2  batch:   61 [ 36600/60000]  losses: ['7.35530329', '0.64025944', '0.17869103']  accuracy train: [' 89.975%', ' 96.407%', ' 95.251%']\n",
      "epoch:  2  batch:   71 [ 42600/60000]  losses: ['6.24482203', '0.47599700', '0.15235525']  accuracy train: [' 89.702%', ' 96.415%', ' 95.256%']\n",
      "epoch:  2  batch:   81 [ 48600/60000]  losses: ['7.75258827', '0.75725132', '0.19154608']  accuracy train: [' 89.508%', ' 96.278%', ' 95.117%']\n",
      "\n",
      "Loss: ['0.62124419', '0.61051345', '0.62104905'] - Accuracy Test: ['78.90', '83.08', '83.71']\n",
      "\n",
      "epoch:  3  batch:    1 [   600/60000]  losses: ['5.10089207', '0.43351400', '0.08400588']  accuracy train: [' 92.500%', ' 98.000%', ' 97.500%']\n",
      "epoch:  3  batch:   11 [  6600/60000]  losses: ['6.82427216', '0.49827111', '0.17582303']  accuracy train: [' 90.970%', ' 96.606%', ' 95.697%']\n",
      "epoch:  3  batch:   21 [ 12600/60000]  losses: ['5.73998022', '0.47805992', '0.10451084']  accuracy train: [' 91.294%', ' 96.873%', ' 95.889%']\n",
      "epoch:  3  batch:   31 [ 18600/60000]  losses: ['5.03796101', '0.43268546', '0.11333944']  accuracy train: [' 91.194%', ' 96.989%', ' 95.925%']\n",
      "epoch:  3  batch:   41 [ 24600/60000]  losses: ['4.72527695', '0.35941783', '0.11549690']  accuracy train: [' 91.346%', ' 97.049%', ' 95.988%']\n",
      "epoch:  3  batch:   51 [ 30600/60000]  losses: ['5.91033030', '0.42806721', '0.12611429']  accuracy train: [' 91.307%', ' 97.108%', ' 95.958%']\n",
      "epoch:  3  batch:   61 [ 36600/60000]  losses: ['5.60910892', '0.35014865', '0.11082246']  accuracy train: [' 91.254%', ' 97.208%', ' 95.904%']\n",
      "epoch:  3  batch:   71 [ 42600/60000]  losses: ['5.54013252', '0.29808876', '0.11066851']  accuracy train: [' 91.258%', ' 97.225%', ' 95.885%']\n",
      "epoch:  3  batch:   81 [ 48600/60000]  losses: ['5.64986181', '0.43084618', '0.11177025']  accuracy train: [' 91.263%', ' 97.272%', ' 95.926%']\n",
      "\n",
      "Loss: ['0.64818984', '0.68160570', '0.91969264'] - Accuracy Test: ['79.80', '83.82', '84.35']\n",
      "\n",
      "epoch:  4  batch:    1 [   600/60000]  losses: ['4.50864029', '0.33240706', '0.08924380']  accuracy train: [' 93.667%', ' 97.833%', ' 97.333%']\n",
      "epoch:  4  batch:   11 [  6600/60000]  losses: ['3.74325252', '0.31371117', '0.09332465']  accuracy train: [' 93.894%', ' 97.970%', ' 96.788%']\n",
      "epoch:  4  batch:   21 [ 12600/60000]  losses: ['3.95508766', '0.29989737', '0.08428688']  accuracy train: [' 93.730%', ' 98.008%', ' 96.897%']\n",
      "epoch:  4  batch:   31 [ 18600/60000]  losses: ['3.89035344', '0.27525562', '0.11797340']  accuracy train: [' 93.871%', ' 98.220%', ' 96.946%']\n",
      "epoch:  4  batch:   41 [ 24600/60000]  losses: ['4.10518742', '0.33557698', '0.10142042']  accuracy train: [' 94.016%', ' 98.301%', ' 96.984%']\n",
      "epoch:  4  batch:   51 [ 30600/60000]  losses: ['4.03321266', '0.18177938', '0.08381442']  accuracy train: [' 94.046%', ' 98.337%', ' 96.974%']\n",
      "epoch:  4  batch:   61 [ 36600/60000]  losses: ['3.79546905', '0.21283647', '0.08829419']  accuracy train: [' 93.948%', ' 98.391%', ' 96.956%']\n",
      "epoch:  4  batch:   71 [ 42600/60000]  losses: ['4.77172661', '0.30890149', '0.08676680']  accuracy train: [' 93.761%', ' 98.383%', ' 96.911%']\n",
      "epoch:  4  batch:   81 [ 48600/60000]  losses: ['4.58035803', '0.25860906', '0.12616140']  accuracy train: [' 93.560%', ' 98.331%', ' 96.794%']\n",
      "\n",
      "Loss: ['0.64785993', '0.62489325', '0.91383886'] - Accuracy Test: ['79.60', '83.98', '82.96']\n",
      "\n",
      "\n",
      "Duration: 150 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train Exits\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "print(model.exit_loss_weights)\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = [0, 0, 0]\n",
    "    tst_corr = [0, 0, 0]\n",
    "    test_count = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)  \n",
    "              \n",
    "        losses = [weighting * criterion(res, y_train) for weighting, res in zip(model.exit_loss_weights, y_pred)]\n",
    "          \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()        \n",
    "        for loss in losses[:-1]:\n",
    "            loss.backward(retain_graph=True)\n",
    "        losses[-1].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for exit, y_pred_exit in enumerate(y_pred):   \n",
    "            predicted = torch.max(y_pred_exit.data, 1)[1]\n",
    "            batch_corr = (predicted == y_train).sum()\n",
    "            trn_corr[exit] += batch_corr\n",
    "        \n",
    "        # Print interim results\n",
    "        if (b-1)%10 == 0:\n",
    "            loss_string = [ f'{loss.item():10.8f}' for loss in losses ]\n",
    "            accu_string = [ f'{correct.item()*100/(batch_size*b):7.3f}%' for correct in trn_corr ]\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000]  losses: {loss_string}  \\\n",
    "accuracy train: {accu_string}')\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            for exit, y_val_exit in enumerate(y_val):\n",
    "                predicted = torch.max(y_val_exit.data, 1)[1]\n",
    "                batch_corr = (predicted == y_test).sum()\n",
    "                tst_corr[exit] += batch_corr            \n",
    "            \n",
    "            test_count += len(predicted)\n",
    "                                  \n",
    "    losses = [ f'{criterion(y_val_exit, y_test).item():10.8f}' for y_val_exit in y_val ]\n",
    "    accs   = [ f'{100*tst_corr_exit/test_count:2.2f}' for tst_corr_exit in tst_corr ]\n",
    "    \n",
    "    print(f\"\\nLoss: {losses} - Accuracy Test: {accs}\\n\")\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
