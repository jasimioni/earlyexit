{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d29764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.AlexNet import AlexNetMNIST, AlexNetMNISTee1, AlexNetMNISTee2\n",
    "from models.Branchynet import Branchynet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649c2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783d9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 600\n",
    "\n",
    "train_data   = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_data    = datasets.FashionMNIST(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef74b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetMNISTee1(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetMNISTee1()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ac6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    break\n",
    "\n",
    "x = X_train.view(1,1,28,28)\n",
    "print(x.shape)\n",
    "x = model.layer1(x)\n",
    "print(x.shape)\n",
    "x = model.layer2(x)\n",
    "print(x.shape)\n",
    "x = model.layer3(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84c5ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:   10 [  6000/60000]  loss: 0.72788686  accuracy:  57.433%\n",
      "epoch:  0  batch:   20 [ 12000/60000]  loss: 0.62489033  accuracy:  66.708%\n",
      "epoch:  0  batch:   30 [ 18000/60000]  loss: 0.52509028  accuracy:  70.656%\n",
      "epoch:  0  batch:   40 [ 24000/60000]  loss: 0.46486497  accuracy:  73.542%\n",
      "epoch:  0  batch:   50 [ 30000/60000]  loss: 0.44731867  accuracy:  75.487%\n",
      "epoch:  0  batch:   60 [ 36000/60000]  loss: 0.43336985  accuracy:  77.000%\n",
      "epoch:  0  batch:   70 [ 42000/60000]  loss: 0.38781312  accuracy:  78.088%\n",
      "epoch:  0  batch:   80 [ 48000/60000]  loss: 0.38396457  accuracy:  79.062%\n",
      "epoch:  0  batch:   90 [ 54000/60000]  loss: 0.34368518  accuracy:  79.961%\n",
      "epoch:  0  batch:  100 [ 60000/60000]  loss: 0.33093280  accuracy:  80.630%\n",
      "epoch:  1  batch:   10 [  6000/60000]  loss: 0.35612792  accuracy:  87.383%\n",
      "epoch:  1  batch:   20 [ 12000/60000]  loss: 0.32843098  accuracy:  87.758%\n",
      "epoch:  1  batch:   30 [ 18000/60000]  loss: 0.27989411  accuracy:  87.833%\n",
      "epoch:  1  batch:   40 [ 24000/60000]  loss: 0.34826559  accuracy:  87.983%\n",
      "epoch:  1  batch:   50 [ 30000/60000]  loss: 0.30716798  accuracy:  87.763%\n",
      "epoch:  1  batch:   60 [ 36000/60000]  loss: 0.26064971  accuracy:  88.047%\n",
      "epoch:  1  batch:   70 [ 42000/60000]  loss: 0.23117030  accuracy:  88.343%\n",
      "epoch:  1  batch:   80 [ 48000/60000]  loss: 0.29388714  accuracy:  88.479%\n",
      "epoch:  1  batch:   90 [ 54000/60000]  loss: 0.26996937  accuracy:  88.607%\n",
      "epoch:  1  batch:  100 [ 60000/60000]  loss: 0.31181249  accuracy:  88.730%\n",
      "epoch:  2  batch:   10 [  6000/60000]  loss: 0.26868555  accuracy:  89.817%\n",
      "epoch:  2  batch:   20 [ 12000/60000]  loss: 0.25358069  accuracy:  90.492%\n",
      "epoch:  2  batch:   30 [ 18000/60000]  loss: 0.21844061  accuracy:  90.639%\n",
      "epoch:  2  batch:   40 [ 24000/60000]  loss: 0.23758212  accuracy:  90.642%\n",
      "epoch:  2  batch:   50 [ 30000/60000]  loss: 0.25876001  accuracy:  90.600%\n",
      "epoch:  2  batch:   60 [ 36000/60000]  loss: 0.28712156  accuracy:  90.525%\n",
      "epoch:  2  batch:   70 [ 42000/60000]  loss: 0.22362480  accuracy:  90.583%\n",
      "epoch:  2  batch:   80 [ 48000/60000]  loss: 0.26775390  accuracy:  90.525%\n",
      "epoch:  2  batch:   90 [ 54000/60000]  loss: 0.21617390  accuracy:  90.648%\n",
      "epoch:  2  batch:  100 [ 60000/60000]  loss: 0.27179441  accuracy:  90.692%\n",
      "epoch:  3  batch:   10 [  6000/60000]  loss: 0.16551246  accuracy:  91.917%\n",
      "epoch:  3  batch:   20 [ 12000/60000]  loss: 0.22487029  accuracy:  92.117%\n",
      "epoch:  3  batch:   30 [ 18000/60000]  loss: 0.26793188  accuracy:  91.817%\n",
      "epoch:  3  batch:   40 [ 24000/60000]  loss: 0.20880754  accuracy:  91.904%\n",
      "epoch:  3  batch:   50 [ 30000/60000]  loss: 0.20852500  accuracy:  91.997%\n",
      "epoch:  3  batch:   60 [ 36000/60000]  loss: 0.24353628  accuracy:  91.828%\n",
      "epoch:  3  batch:   70 [ 42000/60000]  loss: 0.24201587  accuracy:  91.819%\n",
      "epoch:  3  batch:   80 [ 48000/60000]  loss: 0.22990084  accuracy:  91.748%\n",
      "epoch:  3  batch:   90 [ 54000/60000]  loss: 0.24045242  accuracy:  91.661%\n",
      "epoch:  3  batch:  100 [ 60000/60000]  loss: 0.28713396  accuracy:  91.697%\n",
      "epoch:  4  batch:   10 [  6000/60000]  loss: 0.21219125  accuracy:  92.050%\n",
      "epoch:  4  batch:   20 [ 12000/60000]  loss: 0.20678125  accuracy:  92.275%\n",
      "epoch:  4  batch:   30 [ 18000/60000]  loss: 0.26692238  accuracy:  92.478%\n",
      "epoch:  4  batch:   40 [ 24000/60000]  loss: 0.21810636  accuracy:  92.442%\n",
      "epoch:  4  batch:   50 [ 30000/60000]  loss: 0.25384852  accuracy:  92.343%\n",
      "epoch:  4  batch:   60 [ 36000/60000]  loss: 0.16584358  accuracy:  92.400%\n",
      "epoch:  4  batch:   70 [ 42000/60000]  loss: 0.17707010  accuracy:  92.424%\n",
      "epoch:  4  batch:   80 [ 48000/60000]  loss: 0.22771378  accuracy:  92.398%\n",
      "epoch:  4  batch:   90 [ 54000/60000]  loss: 0.21867849  accuracy:  92.337%\n",
      "epoch:  4  batch:  100 [ 60000/60000]  loss: 0.21699534  accuracy:  92.353%\n",
      "\n",
      "Duration: 2595 seconds\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train.cuda()\n",
    "        y_train.cuda()\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if b%10 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(batch_size*b):7.3f}%')\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
