{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d6aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.AlexNet import AlexNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3783bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 600\n",
    "\n",
    "train_data   = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data    = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a0d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    break\n",
    "\n",
    "print(X_train.shape)\n",
    "    \n",
    "x = X_train.view(1,3,32,32)\n",
    "print(x.shape)\n",
    "x = model.layer1(x)\n",
    "print(x.shape)\n",
    "x = model.layer2(x)\n",
    "print(x.shape)\n",
    "x = model.layer3(x)\n",
    "print(x.shape)\n",
    "x = model.layer4(x)\n",
    "print(x.shape)\n",
    "x = model.layer5(x)\n",
    "print(x.shape)\n",
    "x = model.flatten(x)\n",
    "print(x.shape)\n",
    "x = model.fc(x)\n",
    "print(x.shape)\n",
    "x = model.fc1(x)\n",
    "print(x.shape)\n",
    "x = model.fc2(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0146ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:   10 [  6000/60000]  loss: 2.10106063  accuracy:  16.367%\n",
      "epoch:  0  batch:   20 [ 12000/60000]  loss: 1.89759314  accuracy:  20.275%\n",
      "epoch:  0  batch:   30 [ 18000/60000]  loss: 1.77577353  accuracy:  22.717%\n",
      "epoch:  0  batch:   40 [ 24000/60000]  loss: 1.71028197  accuracy:  25.367%\n",
      "epoch:  0  batch:   50 [ 30000/60000]  loss: 1.54277468  accuracy:  27.410%\n",
      "epoch:  0  batch:   60 [ 36000/60000]  loss: 1.53276074  accuracy:  29.672%\n",
      "epoch:  0  batch:   70 [ 42000/60000]  loss: 1.48703694  accuracy:  31.786%\n",
      "epoch:  0  batch:   80 [ 48000/60000]  loss: 1.40437031  accuracy:  33.544%\n",
      "epoch:  1  batch:   10 [  6000/60000]  loss: 1.33883107  accuracy:  49.333%\n",
      "epoch:  1  batch:   20 [ 12000/60000]  loss: 1.27993286  accuracy:  50.408%\n",
      "epoch:  1  batch:   30 [ 18000/60000]  loss: 1.19365561  accuracy:  51.111%\n",
      "epoch:  1  batch:   40 [ 24000/60000]  loss: 1.23480129  accuracy:  51.667%\n",
      "epoch:  1  batch:   50 [ 30000/60000]  loss: 1.16504312  accuracy:  52.293%\n",
      "epoch:  1  batch:   60 [ 36000/60000]  loss: 1.09044755  accuracy:  53.261%\n",
      "epoch:  1  batch:   70 [ 42000/60000]  loss: 1.16917229  accuracy:  53.883%\n",
      "epoch:  1  batch:   80 [ 48000/60000]  loss: 1.10224664  accuracy:  54.552%\n",
      "epoch:  2  batch:   10 [  6000/60000]  loss: 1.00247228  accuracy:  61.617%\n",
      "epoch:  2  batch:   20 [ 12000/60000]  loss: 1.21200252  accuracy:  62.183%\n",
      "epoch:  2  batch:   30 [ 18000/60000]  loss: 1.08479130  accuracy:  62.233%\n",
      "epoch:  2  batch:   40 [ 24000/60000]  loss: 1.00077164  accuracy:  62.817%\n",
      "epoch:  2  batch:   50 [ 30000/60000]  loss: 1.00613511  accuracy:  63.223%\n",
      "epoch:  2  batch:   60 [ 36000/60000]  loss: 1.00719070  accuracy:  63.772%\n",
      "epoch:  2  batch:   70 [ 42000/60000]  loss: 0.97009712  accuracy:  64.224%\n",
      "epoch:  2  batch:   80 [ 48000/60000]  loss: 0.89963591  accuracy:  64.556%\n",
      "epoch:  3  batch:   10 [  6000/60000]  loss: 0.97277296  accuracy:  69.150%\n",
      "epoch:  3  batch:   20 [ 12000/60000]  loss: 0.86170024  accuracy:  70.067%\n",
      "epoch:  3  batch:   30 [ 18000/60000]  loss: 0.86035168  accuracy:  69.756%\n",
      "epoch:  3  batch:   40 [ 24000/60000]  loss: 0.81008130  accuracy:  69.946%\n",
      "epoch:  3  batch:   50 [ 30000/60000]  loss: 0.77917218  accuracy:  69.847%\n",
      "epoch:  3  batch:   60 [ 36000/60000]  loss: 0.86087811  accuracy:  70.086%\n",
      "epoch:  3  batch:   70 [ 42000/60000]  loss: 0.80812114  accuracy:  70.405%\n",
      "epoch:  3  batch:   80 [ 48000/60000]  loss: 0.79269731  accuracy:  70.667%\n",
      "epoch:  4  batch:   10 [  6000/60000]  loss: 0.74815053  accuracy:  71.950%\n",
      "epoch:  4  batch:   20 [ 12000/60000]  loss: 0.73375565  accuracy:  73.008%\n",
      "epoch:  4  batch:   30 [ 18000/60000]  loss: 0.78766596  accuracy:  73.744%\n",
      "epoch:  4  batch:   40 [ 24000/60000]  loss: 0.75775230  accuracy:  73.800%\n",
      "epoch:  4  batch:   50 [ 30000/60000]  loss: 0.67223132  accuracy:  73.987%\n",
      "epoch:  4  batch:   60 [ 36000/60000]  loss: 0.75073111  accuracy:  74.089%\n",
      "epoch:  4  batch:   70 [ 42000/60000]  loss: 0.79761869  accuracy:  74.264%\n",
      "epoch:  4  batch:   80 [ 48000/60000]  loss: 0.64064229  accuracy:  74.575%\n",
      "\n",
      "Duration: 1338 seconds\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train.cuda()\n",
    "        y_train.cuda()\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if b%10 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(batch_size*b):7.3f}%')\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
