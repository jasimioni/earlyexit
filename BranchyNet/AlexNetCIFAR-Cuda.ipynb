{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d6aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from models.AlexNet import AlexNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3783bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 600\n",
    "\n",
    "train_data   = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data    = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a0d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    X_train = X_train.to(device)\n",
    "#    y_train = y_train.to(device)\n",
    "    break\n",
    "\n",
    "print(X_train.shape)\n",
    "    \n",
    "x = X_train.view(1,3,32,32)\n",
    "print(x.shape)\n",
    "x = model.layer1(x)\n",
    "print(x.shape)\n",
    "x = model.layer2(x)\n",
    "print(x.shape)\n",
    "x = model.layer3(x)\n",
    "print(x.shape)\n",
    "x = model.layer4(x)\n",
    "print(x.shape)\n",
    "x = model.layer5(x)\n",
    "print(x.shape)\n",
    "x = model.flatten(x)\n",
    "print(x.shape)\n",
    "x = model.fc(x)\n",
    "print(x.shape)\n",
    "x = model.fc1(x)\n",
    "print(x.shape)\n",
    "x = model.fc2(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0146ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:   10 [  6000/60000]  loss: 0.38162437  accuracy:  85.967%\n",
      "epoch:  0  batch:   20 [ 12000/60000]  loss: 0.43402547  accuracy:  85.992%\n",
      "epoch:  0  batch:   30 [ 18000/60000]  loss: 0.32240543  accuracy:  86.406%\n",
      "epoch:  0  batch:   40 [ 24000/60000]  loss: 0.35413837  accuracy:  86.675%\n",
      "epoch:  0  batch:   50 [ 30000/60000]  loss: 0.43505177  accuracy:  86.703%\n",
      "epoch:  0  batch:   60 [ 36000/60000]  loss: 0.38204834  accuracy:  86.786%\n",
      "epoch:  0  batch:   70 [ 42000/60000]  loss: 0.33155161  accuracy:  86.976%\n",
      "epoch:  0  batch:   80 [ 48000/60000]  loss: 0.40643275  accuracy:  87.150%\n",
      "Loss: 0.63976592 - Accuracy: 81.12\n",
      "epoch:  1  batch:   10 [  6000/60000]  loss: 0.38860798  accuracy:  89.367%\n",
      "epoch:  1  batch:   20 [ 12000/60000]  loss: 0.31322128  accuracy:  89.158%\n",
      "epoch:  1  batch:   30 [ 18000/60000]  loss: 0.26041162  accuracy:  89.417%\n",
      "epoch:  1  batch:   40 [ 24000/60000]  loss: 0.35341981  accuracy:  89.379%\n",
      "epoch:  1  batch:   50 [ 30000/60000]  loss: 0.36575371  accuracy:  89.173%\n",
      "epoch:  1  batch:   60 [ 36000/60000]  loss: 0.34260255  accuracy:  89.033%\n",
      "epoch:  1  batch:   70 [ 42000/60000]  loss: 0.37729341  accuracy:  88.900%\n",
      "epoch:  1  batch:   80 [ 48000/60000]  loss: 0.32158726  accuracy:  88.796%\n",
      "Loss: 0.64882612 - Accuracy: 81.49\n",
      "epoch:  2  batch:   10 [  6000/60000]  loss: 0.30939823  accuracy:  89.750%\n",
      "epoch:  2  batch:   20 [ 12000/60000]  loss: 0.26524118  accuracy:  89.933%\n",
      "epoch:  2  batch:   30 [ 18000/60000]  loss: 0.31122309  accuracy:  89.983%\n",
      "epoch:  2  batch:   40 [ 24000/60000]  loss: 0.33712840  accuracy:  89.971%\n",
      "epoch:  2  batch:   50 [ 30000/60000]  loss: 0.27614567  accuracy:  90.010%\n",
      "epoch:  2  batch:   60 [ 36000/60000]  loss: 0.28247240  accuracy:  90.008%\n",
      "epoch:  2  batch:   70 [ 42000/60000]  loss: 0.25685266  accuracy:  90.083%\n",
      "epoch:  2  batch:   80 [ 48000/60000]  loss: 0.32768342  accuracy:  90.013%\n",
      "Loss: 0.57822412 - Accuracy: 81.96\n",
      "epoch:  3  batch:   10 [  6000/60000]  loss: 0.29109648  accuracy:  90.200%\n",
      "epoch:  3  batch:   20 [ 12000/60000]  loss: 0.25352606  accuracy:  90.767%\n",
      "epoch:  3  batch:   30 [ 18000/60000]  loss: 0.27910993  accuracy:  90.883%\n",
      "epoch:  3  batch:   40 [ 24000/60000]  loss: 0.29705581  accuracy:  90.679%\n",
      "epoch:  3  batch:   50 [ 30000/60000]  loss: 0.31783566  accuracy:  90.477%\n",
      "epoch:  3  batch:   60 [ 36000/60000]  loss: 0.26854542  accuracy:  90.444%\n",
      "epoch:  3  batch:   70 [ 42000/60000]  loss: 0.28257725  accuracy:  90.412%\n",
      "epoch:  3  batch:   80 [ 48000/60000]  loss: 0.28057787  accuracy:  90.485%\n",
      "Loss: 0.62157798 - Accuracy: 82.70\n",
      "epoch:  4  batch:   10 [  6000/60000]  loss: 0.25676239  accuracy:  92.333%\n",
      "epoch:  4  batch:   20 [ 12000/60000]  loss: 0.23192851  accuracy:  92.183%\n",
      "epoch:  4  batch:   30 [ 18000/60000]  loss: 0.20996898  accuracy:  92.150%\n",
      "epoch:  4  batch:   40 [ 24000/60000]  loss: 0.23472027  accuracy:  92.079%\n",
      "epoch:  4  batch:   50 [ 30000/60000]  loss: 0.22945333  accuracy:  91.907%\n",
      "epoch:  4  batch:   60 [ 36000/60000]  loss: 0.20559373  accuracy:  91.875%\n",
      "epoch:  4  batch:   70 [ 42000/60000]  loss: 0.22069873  accuracy:  91.893%\n",
      "epoch:  4  batch:   80 [ 48000/60000]  loss: 0.25668117  accuracy:  91.950%\n",
      "Loss: 0.65367454 - Accuracy: 83.57\n",
      "\n",
      "Duration: 91 seconds\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if b%10 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(batch_size*b):7.3f}%')\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            test_count += len(predicted)\n",
    "            \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "    \n",
    "    print(f\"Loss: {loss.item():10.8f} - Accuracy: {100*tst_corr/test_count:2.2f}\")\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
