{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2bfa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.AlexNet import AlexNetMNIST, AlexNetMNISTee1, AlexNetMNISTee2, AlexNetWithExistsMNIST\n",
    "from models.Branchynet import Branchynet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee824f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNetWithExistsMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4908fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 600\n",
    "\n",
    "train_data   = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_data    = datasets.FashionMNIST(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab9ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:    1 [   600/60000] losses: ['2.59961367', '1.26119518', '0.46754408']\n",
      "epoch:  0  batch:    2 [  1200/60000] losses: ['2.23671889', '1.25556040', '1.34791768']\n",
      "epoch:  0  batch:    3 [  1800/60000] losses: ['1.82373369', '1.15802646', '1.04345906']\n",
      "epoch:  0  batch:    4 [  2400/60000] losses: ['1.20215595', '0.72629648', '0.70451087']\n",
      "epoch:  0  batch:    5 [  3000/60000] losses: ['1.17375028', '0.56360120', '0.55334598']\n",
      "epoch:  0  batch:    6 [  3600/60000] losses: ['1.02933168', '0.55307651', '0.46745703']\n",
      "epoch:  0  batch:    7 [  4200/60000] losses: ['1.03842342', '0.54510200', '0.40105301']\n",
      "epoch:  0  batch:    8 [  4800/60000] losses: ['0.95952982', '0.48803285', '0.39068148']\n",
      "epoch:  0  batch:    9 [  5400/60000] losses: ['0.79567772', '0.43787125', '0.36351272']\n",
      "epoch:  0  batch:   10 [  6000/60000] losses: ['0.77403730', '0.43133774', '0.33857253']\n",
      "epoch:  0  batch:   11 [  6600/60000] losses: ['0.79140180', '0.43166468', '0.31460717']\n",
      "epoch:  0  batch:   12 [  7200/60000] losses: ['0.67360312', '0.37440163', '0.30693063']\n",
      "epoch:  0  batch:   13 [  7800/60000] losses: ['0.67371535', '0.34212831', '0.26405281']\n",
      "epoch:  0  batch:   14 [  8400/60000] losses: ['0.76372492', '0.40854597', '0.24464889']\n",
      "epoch:  0  batch:   15 [  9000/60000] losses: ['0.67916137', '0.36427224', '0.24002433']\n",
      "epoch:  0  batch:   16 [  9600/60000] losses: ['0.68254954', '0.36623791', '0.22554989']\n",
      "epoch:  0  batch:   17 [ 10200/60000] losses: ['0.61324215', '0.31262803', '0.18666042']\n",
      "epoch:  0  batch:   18 [ 10800/60000] losses: ['0.58951801', '0.30311036', '0.19339833']\n",
      "epoch:  0  batch:   19 [ 11400/60000] losses: ['0.65462065', '0.32073528', '0.19723712']\n",
      "epoch:  0  batch:   20 [ 12000/60000] losses: ['0.60903311', '0.31218359', '0.19040154']\n",
      "epoch:  0  batch:   21 [ 12600/60000] losses: ['0.59092808', '0.28933489', '0.19186300']\n",
      "epoch:  0  batch:   22 [ 13200/60000] losses: ['0.59525806', '0.28750721', '0.17738722']\n",
      "epoch:  0  batch:   23 [ 13800/60000] losses: ['0.62126213', '0.31960234', '0.19304799']\n",
      "epoch:  0  batch:   24 [ 14400/60000] losses: ['0.60805446', '0.30444235', '0.17214178']\n",
      "epoch:  0  batch:   25 [ 15000/60000] losses: ['0.49306309', '0.24443960', '0.13862003']\n",
      "epoch:  0  batch:   26 [ 15600/60000] losses: ['0.51136547', '0.27424332', '0.16104655']\n",
      "epoch:  0  batch:   27 [ 16200/60000] losses: ['0.53855640', '0.29040813', '0.15512927']\n",
      "epoch:  0  batch:   28 [ 16800/60000] losses: ['0.57331312', '0.29472333', '0.16292678']\n",
      "epoch:  0  batch:   29 [ 17400/60000] losses: ['0.56390643', '0.29290271', '0.15202858']\n",
      "epoch:  0  batch:   30 [ 18000/60000] losses: ['0.48810267', '0.24487714', '0.14844899']\n",
      "epoch:  0  batch:   31 [ 18600/60000] losses: ['0.53903478', '0.27438581', '0.15419465']\n",
      "epoch:  0  batch:   32 [ 19200/60000] losses: ['0.54390681', '0.27827507', '0.15440257']\n",
      "epoch:  0  batch:   33 [ 19800/60000] losses: ['0.57078928', '0.29037732', '0.15715702']\n",
      "epoch:  0  batch:   34 [ 20400/60000] losses: ['0.44970942', '0.23121186', '0.14218858']\n",
      "epoch:  0  batch:   35 [ 21000/60000] losses: ['0.56227881', '0.28699544', '0.16216525']\n",
      "epoch:  0  batch:   36 [ 21600/60000] losses: ['0.54638934', '0.28106812', '0.14526521']\n",
      "epoch:  0  batch:   37 [ 22200/60000] losses: ['0.47466862', '0.25166789', '0.13794199']\n",
      "epoch:  0  batch:   38 [ 22800/60000] losses: ['0.46607554', '0.22996348', '0.13208489']\n",
      "epoch:  0  batch:   39 [ 23400/60000] losses: ['0.46426290', '0.23713849', '0.13185704']\n",
      "epoch:  0  batch:   40 [ 24000/60000] losses: ['0.49183136', '0.25395471', '0.14244674']\n",
      "epoch:  0  batch:   41 [ 24600/60000] losses: ['0.47606155', '0.24050105', '0.12941341']\n",
      "epoch:  0  batch:   42 [ 25200/60000] losses: ['0.46401560', '0.23143201', '0.13137750']\n",
      "epoch:  0  batch:   43 [ 25800/60000] losses: ['0.50937486', '0.26856726', '0.14348078']\n",
      "epoch:  0  batch:   44 [ 26400/60000] losses: ['0.43477860', '0.21140596', '0.12070280']\n",
      "epoch:  0  batch:   45 [ 27000/60000] losses: ['0.45017019', '0.23015401', '0.13141058']\n",
      "epoch:  0  batch:   46 [ 27600/60000] losses: ['0.41928601', '0.21209347', '0.11549105']\n",
      "epoch:  0  batch:   47 [ 28200/60000] losses: ['0.49032658', '0.25168157', '0.13590291']\n",
      "epoch:  0  batch:   48 [ 28800/60000] losses: ['0.46111608', '0.22599174', '0.12769893']\n",
      "epoch:  0  batch:   49 [ 29400/60000] losses: ['0.47620240', '0.23702531', '0.12881929']\n",
      "epoch:  0  batch:   50 [ 30000/60000] losses: ['0.40559042', '0.20724684', '0.11269685']\n",
      "epoch:  0  batch:   51 [ 30600/60000] losses: ['0.41549468', '0.21308342', '0.12158014']\n",
      "epoch:  0  batch:   52 [ 31200/60000] losses: ['0.44910386', '0.22483905', '0.12482341']\n",
      "epoch:  0  batch:   53 [ 31800/60000] losses: ['0.43791026', '0.21651995', '0.12114298']\n",
      "epoch:  0  batch:   54 [ 32400/60000] losses: ['0.39770612', '0.20454031', '0.11125249']\n",
      "epoch:  0  batch:   55 [ 33000/60000] losses: ['0.39969566', '0.20423862', '0.10896504']\n",
      "epoch:  0  batch:   56 [ 33600/60000] losses: ['0.47884303', '0.23585694', '0.13028084']\n",
      "epoch:  0  batch:   57 [ 34200/60000] losses: ['0.46319306', '0.23495418', '0.12345463']\n",
      "epoch:  0  batch:   58 [ 34800/60000] losses: ['0.43438029', '0.21179001', '0.11206957']\n",
      "epoch:  0  batch:   59 [ 35400/60000] losses: ['0.42852134', '0.22169355', '0.11865748']\n",
      "epoch:  0  batch:   60 [ 36000/60000] losses: ['0.46187621', '0.25541618', '0.12629284']\n",
      "epoch:  0  batch:   61 [ 36600/60000] losses: ['0.35944709', '0.17320906', '0.10354506']\n",
      "epoch:  0  batch:   62 [ 37200/60000] losses: ['0.39099032', '0.20227619', '0.11255302']\n",
      "epoch:  0  batch:   63 [ 37800/60000] losses: ['0.40243810', '0.21315354', '0.11668413']\n",
      "epoch:  0  batch:   64 [ 38400/60000] losses: ['0.34689444', '0.18036765', '0.10813677']\n",
      "epoch:  0  batch:   65 [ 39000/60000] losses: ['0.41007587', '0.20818312', '0.10264124']\n",
      "epoch:  0  batch:   66 [ 39600/60000] losses: ['0.42002866', '0.20118977', '0.10703363']\n",
      "epoch:  0  batch:   67 [ 40200/60000] losses: ['0.38179120', '0.19571298', '0.10877048']\n",
      "epoch:  0  batch:   68 [ 40800/60000] losses: ['0.38645545', '0.19236197', '0.10531497']\n",
      "epoch:  0  batch:   69 [ 41400/60000] losses: ['0.37180719', '0.18936557', '0.11264499']\n",
      "epoch:  0  batch:   70 [ 42000/60000] losses: ['0.35380125', '0.17508857', '0.10091782']\n",
      "epoch:  0  batch:   71 [ 42600/60000] losses: ['0.44690186', '0.21929573', '0.11294394']\n",
      "epoch:  0  batch:   72 [ 43200/60000] losses: ['0.36185181', '0.17822741', '0.10273414']\n",
      "epoch:  0  batch:   73 [ 43800/60000] losses: ['0.41235080', '0.20027643', '0.11099749']\n",
      "epoch:  0  batch:   74 [ 44400/60000] losses: ['0.39402917', '0.19044513', '0.10626914']\n",
      "epoch:  0  batch:   75 [ 45000/60000] losses: ['0.39708477', '0.19588964', '0.11164161']\n",
      "epoch:  0  batch:   76 [ 45600/60000] losses: ['0.37478629', '0.18113445', '0.10195187']\n",
      "epoch:  0  batch:   77 [ 46200/60000] losses: ['0.39712632', '0.18949430', '0.09809664']\n",
      "epoch:  0  batch:   78 [ 46800/60000] losses: ['0.35491258', '0.18593453', '0.10827229']\n",
      "epoch:  0  batch:   79 [ 47400/60000] losses: ['0.34609544', '0.17256902', '0.10541718']\n",
      "epoch:  0  batch:   80 [ 48000/60000] losses: ['0.39574650', '0.21178263', '0.11433422']\n",
      "epoch:  0  batch:   81 [ 48600/60000] losses: ['0.34967709', '0.17410500', '0.10264365']\n",
      "epoch:  0  batch:   82 [ 49200/60000] losses: ['0.44150853', '0.21773820', '0.11669395']\n",
      "epoch:  0  batch:   83 [ 49800/60000] losses: ['0.32311726', '0.17067277', '0.10397493']\n",
      "epoch:  0  batch:   84 [ 50400/60000] losses: ['0.45872316', '0.22491638', '0.12237420']\n",
      "epoch:  0  batch:   85 [ 51000/60000] losses: ['0.36011487', '0.19371533', '0.09543844']\n",
      "epoch:  0  batch:   86 [ 51600/60000] losses: ['0.35066578', '0.17713654', '0.10507449']\n",
      "epoch:  0  batch:   87 [ 52200/60000] losses: ['0.34907293', '0.17646003', '0.09926438']\n",
      "epoch:  0  batch:   88 [ 52800/60000] losses: ['0.35963848', '0.19007996', '0.10521060']\n",
      "epoch:  0  batch:   89 [ 53400/60000] losses: ['0.32049111', '0.16267110', '0.09121136']\n",
      "epoch:  0  batch:   90 [ 54000/60000] losses: ['0.35024613', '0.16398816', '0.09280821']\n",
      "epoch:  0  batch:   91 [ 54600/60000] losses: ['0.35260147', '0.18533358', '0.09580879']\n",
      "epoch:  0  batch:   92 [ 55200/60000] losses: ['0.31670395', '0.15605366', '0.09410080']\n",
      "epoch:  0  batch:   93 [ 55800/60000] losses: ['0.33889794', '0.16693343', '0.09563202']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:   94 [ 56400/60000] losses: ['0.30520639', '0.15357922', '0.09193730']\n",
      "epoch:  0  batch:   95 [ 57000/60000] losses: ['0.36080912', '0.17826840', '0.10741962']\n",
      "epoch:  0  batch:   96 [ 57600/60000] losses: ['0.35568300', '0.18522987', '0.10675906']\n",
      "epoch:  0  batch:   97 [ 58200/60000] losses: ['0.35643065', '0.17041691', '0.09612516']\n",
      "epoch:  0  batch:   98 [ 58800/60000] losses: ['0.39374104', '0.19748822', '0.11019895']\n",
      "epoch:  0  batch:   99 [ 59400/60000] losses: ['0.32799962', '0.16513370', '0.08706550']\n",
      "epoch:  0  batch:  100 [ 60000/60000] losses: ['0.34087148', '0.17266971', '0.08884279']\n",
      "Accuracy: 87.01000213623047 86.69000244140625 81.30000305175781\n",
      "epoch:  1  batch:    1 [   600/60000] losses: ['0.34987250', '0.18867518', '0.09644701']\n",
      "epoch:  1  batch:    2 [  1200/60000] losses: ['0.39042497', '0.19867963', '0.10595768']\n",
      "epoch:  1  batch:    3 [  1800/60000] losses: ['0.32744256', '0.16036533', '0.08453111']\n",
      "epoch:  1  batch:    4 [  2400/60000] losses: ['0.32948932', '0.16868040', '0.10094587']\n",
      "epoch:  1  batch:    5 [  3000/60000] losses: ['0.30543664', '0.15669633', '0.08713186']\n",
      "epoch:  1  batch:    6 [  3600/60000] losses: ['0.40046194', '0.19914435', '0.09860212']\n",
      "epoch:  1  batch:    7 [  4200/60000] losses: ['0.36592966', '0.18171054', '0.08966332']\n",
      "epoch:  1  batch:    8 [  4800/60000] losses: ['0.31546825', '0.16156979', '0.08518761']\n",
      "epoch:  1  batch:    9 [  5400/60000] losses: ['0.29888734', '0.14834040', '0.08214503']\n",
      "epoch:  1  batch:   10 [  6000/60000] losses: ['0.42198259', '0.19880654', '0.09920929']\n",
      "epoch:  1  batch:   11 [  6600/60000] losses: ['0.32500637', '0.16746904', '0.08580732']\n",
      "epoch:  1  batch:   12 [  7200/60000] losses: ['0.30606660', '0.14983380', '0.09101759']\n",
      "epoch:  1  batch:   13 [  7800/60000] losses: ['0.30059552', '0.15025671', '0.08295941']\n",
      "epoch:  1  batch:   14 [  8400/60000] losses: ['0.34253129', '0.16557987', '0.08413036']\n",
      "epoch:  1  batch:   15 [  9000/60000] losses: ['0.34943575', '0.16698919', '0.08493059']\n",
      "epoch:  1  batch:   16 [  9600/60000] losses: ['0.32117566', '0.16463710', '0.08143251']\n",
      "epoch:  1  batch:   17 [ 10200/60000] losses: ['0.41905013', '0.19097635', '0.09815341']\n",
      "epoch:  1  batch:   18 [ 10800/60000] losses: ['0.35336623', '0.18110065', '0.08880959']\n",
      "epoch:  1  batch:   19 [ 11400/60000] losses: ['0.39814752', '0.19482282', '0.09538391']\n",
      "epoch:  1  batch:   20 [ 12000/60000] losses: ['0.38457176', '0.17652498', '0.08553948']\n",
      "epoch:  1  batch:   21 [ 12600/60000] losses: ['0.40151745', '0.18556769', '0.10081186']\n",
      "epoch:  1  batch:   22 [ 13200/60000] losses: ['0.36604428', '0.16815953', '0.09887860']\n",
      "epoch:  1  batch:   23 [ 13800/60000] losses: ['0.35673177', '0.16418308', '0.09980412']\n",
      "epoch:  1  batch:   24 [ 14400/60000] losses: ['0.37705344', '0.17449377', '0.08716238']\n",
      "epoch:  1  batch:   25 [ 15000/60000] losses: ['0.32288396', '0.13825531', '0.08262518']\n",
      "epoch:  1  batch:   26 [ 15600/60000] losses: ['0.31964770', '0.15214621', '0.08739465']\n",
      "epoch:  1  batch:   27 [ 16200/60000] losses: ['0.30336827', '0.15203829', '0.08517811']\n",
      "epoch:  1  batch:   28 [ 16800/60000] losses: ['0.33393049', '0.16657794', '0.09181669']\n",
      "epoch:  1  batch:   29 [ 17400/60000] losses: ['0.38337946', '0.17791308', '0.09220145']\n",
      "epoch:  1  batch:   30 [ 18000/60000] losses: ['0.26857203', '0.13697849', '0.07294487']\n",
      "epoch:  1  batch:   31 [ 18600/60000] losses: ['0.31149092', '0.14238700', '0.07592280']\n",
      "epoch:  1  batch:   32 [ 19200/60000] losses: ['0.34925061', '0.16564018', '0.09230311']\n",
      "epoch:  1  batch:   33 [ 19800/60000] losses: ['0.33227247', '0.16536838', '0.07285871']\n",
      "epoch:  1  batch:   34 [ 20400/60000] losses: ['0.31924939', '0.15182605', '0.07813092']\n",
      "epoch:  1  batch:   35 [ 21000/60000] losses: ['0.33667216', '0.16522722', '0.09101063']\n",
      "epoch:  1  batch:   36 [ 21600/60000] losses: ['0.30352479', '0.15919545', '0.07298049']\n",
      "epoch:  1  batch:   37 [ 22200/60000] losses: ['0.39628527', '0.19552295', '0.09608311']\n",
      "epoch:  1  batch:   38 [ 22800/60000] losses: ['0.28410593', '0.14994812', '0.08519976']\n",
      "epoch:  1  batch:   39 [ 23400/60000] losses: ['0.33846051', '0.15949412', '0.08135212']\n",
      "epoch:  1  batch:   40 [ 24000/60000] losses: ['0.34761727', '0.17893541', '0.09493621']\n",
      "epoch:  1  batch:   41 [ 24600/60000] losses: ['0.27936780', '0.13737798', '0.07192355']\n",
      "epoch:  1  batch:   42 [ 25200/60000] losses: ['0.35489714', '0.15989685', '0.08159497']\n",
      "epoch:  1  batch:   43 [ 25800/60000] losses: ['0.24334458', '0.12746590', '0.07326645']\n",
      "epoch:  1  batch:   44 [ 26400/60000] losses: ['0.31931150', '0.16338988', '0.08459443']\n",
      "epoch:  1  batch:   45 [ 27000/60000] losses: ['0.34920004', '0.14344366', '0.07900098']\n",
      "epoch:  1  batch:   46 [ 27600/60000] losses: ['0.29635704', '0.14393930', '0.07246633']\n",
      "epoch:  1  batch:   47 [ 28200/60000] losses: ['0.31306645', '0.15841523', '0.08452595']\n",
      "epoch:  1  batch:   48 [ 28800/60000] losses: ['0.30576599', '0.15041226', '0.07489927']\n",
      "epoch:  1  batch:   49 [ 29400/60000] losses: ['0.33548605', '0.16074489', '0.08215600']\n",
      "epoch:  1  batch:   50 [ 30000/60000] losses: ['0.36740237', '0.16632834', '0.08634979']\n",
      "epoch:  1  batch:   51 [ 30600/60000] losses: ['0.30382273', '0.16305944', '0.08034316']\n",
      "epoch:  1  batch:   52 [ 31200/60000] losses: ['0.21282722', '0.10656441', '0.05966886']\n",
      "epoch:  1  batch:   53 [ 31800/60000] losses: ['0.29236645', '0.13954423', '0.07459076']\n",
      "epoch:  1  batch:   54 [ 32400/60000] losses: ['0.39464527', '0.19275562', '0.08889236']\n",
      "epoch:  1  batch:   55 [ 33000/60000] losses: ['0.35128292', '0.18162030', '0.08690824']\n",
      "epoch:  1  batch:   56 [ 33600/60000] losses: ['0.31503174', '0.15814893', '0.07926391']\n",
      "epoch:  1  batch:   57 [ 34200/60000] losses: ['0.31725773', '0.15310194', '0.07455833']\n",
      "epoch:  1  batch:   58 [ 34800/60000] losses: ['0.27460626', '0.13880855', '0.07537906']\n",
      "epoch:  1  batch:   59 [ 35400/60000] losses: ['0.32306463', '0.16386901', '0.08298408']\n",
      "epoch:  1  batch:   60 [ 36000/60000] losses: ['0.35056913', '0.16409762', '0.08616064']\n",
      "epoch:  1  batch:   61 [ 36600/60000] losses: ['0.29997489', '0.15565245', '0.07010935']\n",
      "epoch:  1  batch:   62 [ 37200/60000] losses: ['0.29710525', '0.14545226', '0.07582066']\n",
      "epoch:  1  batch:   63 [ 37800/60000] losses: ['0.34348947', '0.14935441', '0.08042098']\n",
      "epoch:  1  batch:   64 [ 38400/60000] losses: ['0.25947505', '0.12538384', '0.06770661']\n",
      "epoch:  1  batch:   65 [ 39000/60000] losses: ['0.28805661', '0.14518690', '0.07592162']\n",
      "epoch:  1  batch:   66 [ 39600/60000] losses: ['0.29359400', '0.13582963', '0.07525807']\n",
      "epoch:  1  batch:   67 [ 40200/60000] losses: ['0.29726645', '0.15039773', '0.07575310']\n",
      "epoch:  1  batch:   68 [ 40800/60000] losses: ['0.25200912', '0.12974009', '0.07113182']\n",
      "epoch:  1  batch:   69 [ 41400/60000] losses: ['0.30648065', '0.14939274', '0.08138578']\n",
      "epoch:  1  batch:   70 [ 42000/60000] losses: ['0.23720112', '0.13091260', '0.06604151']\n",
      "epoch:  1  batch:   71 [ 42600/60000] losses: ['0.29773998', '0.14890227', '0.07820811']\n",
      "epoch:  1  batch:   72 [ 43200/60000] losses: ['0.30336085', '0.14554553', '0.07540444']\n",
      "epoch:  1  batch:   73 [ 43800/60000] losses: ['0.32951128', '0.15685754', '0.07829798']\n",
      "epoch:  1  batch:   74 [ 44400/60000] losses: ['0.28069532', '0.12607121', '0.06603017']\n",
      "epoch:  1  batch:   75 [ 45000/60000] losses: ['0.28688139', '0.14837541', '0.08001223']\n",
      "epoch:  1  batch:   76 [ 45600/60000] losses: ['0.28337678', '0.14645179', '0.06746047']\n",
      "epoch:  1  batch:   77 [ 46200/60000] losses: ['0.29999349', '0.14707993', '0.07199957']\n",
      "epoch:  1  batch:   78 [ 46800/60000] losses: ['0.31451094', '0.17104904', '0.08216746']\n",
      "epoch:  1  batch:   79 [ 47400/60000] losses: ['0.30190688', '0.14310953', '0.07187280']\n",
      "epoch:  1  batch:   80 [ 48000/60000] losses: ['0.24571261', '0.12428030', '0.06621892']\n",
      "epoch:  1  batch:   81 [ 48600/60000] losses: ['0.26576778', '0.12462042', '0.05603717']\n",
      "epoch:  1  batch:   82 [ 49200/60000] losses: ['0.28183946', '0.15108669', '0.07297976']\n",
      "epoch:  1  batch:   83 [ 49800/60000] losses: ['0.30453163', '0.15200493', '0.07012668']\n",
      "epoch:  1  batch:   84 [ 50400/60000] losses: ['0.29861301', '0.15117013', '0.07342414']\n",
      "epoch:  1  batch:   85 [ 51000/60000] losses: ['0.26048601', '0.13111940', '0.06280760']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  batch:   86 [ 51600/60000] losses: ['0.32138067', '0.14982344', '0.07479569']\n",
      "epoch:  1  batch:   87 [ 52200/60000] losses: ['0.21972409', '0.10537992', '0.05459685']\n",
      "epoch:  1  batch:   88 [ 52800/60000] losses: ['0.29449669', '0.14360201', '0.06717978']\n",
      "epoch:  1  batch:   89 [ 53400/60000] losses: ['0.24091141', '0.10800074', '0.05743914']\n",
      "epoch:  1  batch:   90 [ 54000/60000] losses: ['0.29865485', '0.14855437', '0.06821623']\n",
      "epoch:  1  batch:   91 [ 54600/60000] losses: ['0.24656986', '0.12543873', '0.06088475']\n",
      "epoch:  1  batch:   92 [ 55200/60000] losses: ['0.25038266', '0.12558013', '0.06152388']\n",
      "epoch:  1  batch:   93 [ 55800/60000] losses: ['0.24352859', '0.11645695', '0.06344802']\n",
      "epoch:  1  batch:   94 [ 56400/60000] losses: ['0.28710747', '0.14414020', '0.07153853']\n",
      "epoch:  1  batch:   95 [ 57000/60000] losses: ['0.31194627', '0.15368934', '0.07020823']\n",
      "epoch:  1  batch:   96 [ 57600/60000] losses: ['0.26719341', '0.13384138', '0.06593625']\n",
      "epoch:  1  batch:   97 [ 58200/60000] losses: ['0.29942256', '0.13765365', '0.06841989']\n",
      "epoch:  1  batch:   98 [ 58800/60000] losses: ['0.26940835', '0.14127664', '0.07039168']\n",
      "epoch:  1  batch:   99 [ 59400/60000] losses: ['0.25078937', '0.12783079', '0.06464841']\n",
      "epoch:  1  batch:  100 [ 60000/60000] losses: ['0.30275360', '0.15055659', '0.06699460']\n",
      "Accuracy: 89.29000091552734 88.94999694824219 87.19000244140625\n",
      "epoch:  2  batch:    1 [   600/60000] losses: ['0.25557318', '0.12767406', '0.06175367']\n",
      "epoch:  2  batch:    2 [  1200/60000] losses: ['0.32102177', '0.14782013', '0.07039390']\n",
      "epoch:  2  batch:    3 [  1800/60000] losses: ['0.23453583', '0.11935126', '0.06688548']\n",
      "epoch:  2  batch:    4 [  2400/60000] losses: ['0.26159212', '0.12615557', '0.07061239']\n",
      "epoch:  2  batch:    5 [  3000/60000] losses: ['0.29023525', '0.14260824', '0.07083262']\n",
      "epoch:  2  batch:    6 [  3600/60000] losses: ['0.24760368', '0.12130207', '0.06114858']\n",
      "epoch:  2  batch:    7 [  4200/60000] losses: ['0.26514059', '0.12940243', '0.06497463']\n",
      "epoch:  2  batch:    8 [  4800/60000] losses: ['0.30892834', '0.16904815', '0.08228144']\n",
      "epoch:  2  batch:    9 [  5400/60000] losses: ['0.29818910', '0.16111040', '0.07559586']\n",
      "epoch:  2  batch:   10 [  6000/60000] losses: ['0.22778954', '0.12329672', '0.05200536']\n",
      "epoch:  2  batch:   11 [  6600/60000] losses: ['0.28490981', '0.14793868', '0.06272730']\n",
      "epoch:  2  batch:   12 [  7200/60000] losses: ['0.27679074', '0.13036646', '0.07050138']\n",
      "epoch:  2  batch:   13 [  7800/60000] losses: ['0.28940400', '0.14890479', '0.05949970']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train\n",
    "    \n",
    "    for b, (xb, yb) in enumerate(train_loader):\n",
    "        b+=1\n",
    "\n",
    "        results = model(xb)\n",
    "        losses = [weighting * criterion(res, yb)\n",
    "                        for weighting, res in zip(model.exit_loss_weights,results)]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for loss in losses[:-1]:\n",
    "            loss.backward(retain_graph=True)\n",
    "        losses[-1].backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if b%1 == 0:\n",
    "            print_losses = list(map(lambda x: f'{x.item():10.8f}', losses))\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/60000] losses: {print_losses}')\n",
    "                \n",
    "    correct_per_exit = [0, 0, 0]\n",
    "    total_tests = 0\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            \n",
    "            # Apply the model\n",
    "            y_val = model(X_test)\n",
    "            \n",
    "            for n, exit in enumerate(y_val):\n",
    "                predicted = torch.max(exit.data, 1)[1] \n",
    "                correct_per_exit[n] += (predicted == y_test).sum()\n",
    "            \n",
    "            total_tests += len(y_test)\n",
    "            \n",
    "    print(f'Accuracy: {100*correct_per_exit[0]/total_tests} {100*correct_per_exit[1]/total_tests} {100*correct_per_exit[2]/total_tests}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
